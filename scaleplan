You are the lead scalability engineer for Troll City. Goal: build a “TikTok-level” architecture where the codebase can scale from launch through viral traffic without rewrites. Money/upgrades are not the issue. Design must support 10k→100k→1M users. Key: WebRTC for interaction, HLS via CDN for mass viewers, and an event plane that is NOT Postgres changefeeds. Implement the foundational seams now.

HARD CONSTRAINTS (NON-NEGOTIABLE)

No viewer subscriptions to Postgres postgres_changes for chat or gifts. Postgres is durability only.

All coin/gift actions are server-authoritative, transactional, and idempotent. No direct client writes to ledger/gifts.

Hybrid media delivery is required:

WebRTC (LiveKit) for broadcaster + guests (interactive)

HLS for massive audiences (viral streams)

Graceful degradation under load: slow mode, throttles, dropping non-critical events, never “crash.”

Observability-first: we must measure msgs/sec, gifts/sec, p95/p99 latencies, error rates, and media bandwidth.

PART A — HYBRID WEBRTC + HLS (VIRAL SAFE)
Objective

A stream can go viral (10k–100k viewers) without LiveKit SFU collapse. WebRTC remains for host/guests and optionally a small interactive subset; mass viewers use HLS delivered over CDN.

Implement

Add DB fields to the streams table:

delivery_mode: webrtc | hls | hybrid

hls_playback_url (nullable)

hls_started_at (nullable)

viewer_count_live (ephemeral/cached; DO NOT write per join/leave)

hls_threshold_viewers (config; default e.g. 500)

hls_force_threshold_viewers (config; default e.g. 2000)

Add server orchestrator (Edge Function or backend service) “Stream Orchestrator”:

Periodically evaluates stream viewer counts (from LiveKit room participants or presence)

When viewer_count ≥ threshold:

Starts HLS egress pipeline for that stream

Persists hls_playback_url to DB

Broadcasts event hls_ready to stream_events_${streamId}

When stream ends:

Stops egress, marks ended, optionally triggers VOD packaging

HLS generation method:

Use LiveKit egress to produce HLS segments + master playlist.

Store segments on a scalable object store (choose default if needed: Cloudflare R2 or AWS S3).

Serve HLS via CDN (Bunny can serve static, but ensure video delivery is configured properly; CDN must support range requests and high throughput).

Frontend player switching:

Default join mode:

If delivery_mode=webrtc OR no hls_playback_url: join WebRTC room as viewer (subject to caps)

If hls_playback_url exists OR viewer_count ≥ force threshold: use HLS player

On hls_ready broadcast:

Non-interactive viewers auto-switch to HLS seamlessly

Maintain interactive capability:

Host and guests stay WebRTC always

Optionally allow a limited “interactive seats” count for VIP viewers (cap)

Deliverables:

DB migration for new stream fields

Orchestrator function/service + start/stop egress

Client updates to support WebRTC player + HLS player + live switching

Ensure no downtime and no broken routes

PART B — REALTIME EVENTS (CHAT/GIFTS) AS EVENT PLANE, NOT DB CHANGEFEEDS
Objective

Realtime fanout must not scale with Postgres subscriptions.

Implement “Event API” seam in frontend

Create a module used by all UI:

subscribeStreamEvents(streamId)

publishStreamEvent(streamId, event)

unsubscribe()

Backend transport today: Supabase Realtime Broadcast (channel stream_events_${streamId}).
Future: plug-in event bus without touching UI.

Chat requirements

Viewers: subscribe ONLY to broadcast chat_message.

Durability: chat still inserts into messages table.

Catch-up:

On join/reconnect, fetch last N messages from DB (50–200) and then resume broadcast.

Ordering/dedupe:

Include message_id, created_at, dedupe by id.

Rate limits + slow mode (server enforced):

per user: 1 msg / 1–2 sec default

per room: msgs/sec cap; if exceeded broadcast slow_mode_enabled.

Implementation detail:

Client calls server endpoint send_chat_message(streamId, text, client_msg_id) (Edge Function preferred).

Server validates, inserts, returns canonical id, broadcasts after commit (idempotent by client_msg_id).

Gifts requirements

Realtime gift animation: broadcast gift_sent only.

Durability: server writes ledger + gift record.

Remove viewer DB insert listeners (no postgres_changes for gifts).

Throttle/queue:

per user gifts/sec + per room gifts/sec caps.

Deliverables:

Refactor ChatBox.tsx, ChatOverlay.tsx, mobile chat components: broadcast-first; remove postgres_changes for viewers.

Refactor useGiftEvents.ts: broadcast-only for realtime UI; DB listeners only for admin/debug mode.

PART C — MONEY / LEDGER SYSTEM (BANK CORRECT + SCALABLE)
Objective

Prevent double-spend and allow high gifts/sec without DB meltdown.

Implement a single server-side gift pipeline:

Endpoint/RPC: process_gift(streamId, receiverId, giftTypeId, amount, client_txn_id)

Must be transactional:

debit sender

credit receiver (or pending bucket)

insert ledger rows

insert gift row

minimal synchronous aggregate updates (2–4 writes ideal)

Must be idempotent:

unique constraint on (sender_id, client_txn_id)

retries return the same result without double debit

After commit:

broadcast gift_sent with canonical payload

Heavy aggregates (leaderboards, stream totals) go async via queue/worker.

Deliverables:

DB constraints for idempotency

Transactional function with minimal writes

Async aggregation worker + queue (Upstash Redis or similar)

Read models:

user_balances

stream_stats

battle_stats
UI must read from read models, not scan ledger tables.

PART D — PRESENCE / VIEWER COUNT (NO DB WRITES PER EVENT)

Viewer counts must come from:

LiveKit participant counts, or

ephemeral presence in event plane
Store only periodic snapshots (every 10–30s) if needed.

Deliverables:

Remove any DB writes per join/leave

Provide stable viewer count API backed by cache

PART E — PERFORMANCE: CACHING + FEED READ MODELS

Implement caching layer (Redis) for:

explore/live lists

profiles

leaderboards

stream metadata
Use cursor pagination for feeds.

Deliverables:

cache module + TTL strategy + invalidation on stream start/end and key events

PART F — OBSERVABILITY + LOAD TESTING

We need real capacity numbers, not guesses.

Add dashboards/alerts:

msgs/sec per room + global

gifts/sec per room + global

Edge Function p95/p99

DB p95/p99

broadcast publish failure rate

LiveKit: participants, rooms, bandwidth, CPU
Alerts at 70/85/95%.

Build a load harness that can simulate:

10k viewers on HLS for a viral stream

1k interactive WebRTC viewers (tiered)

chat storms + gift storms
System must degrade via slow mode/throttle, not crash.

ACCEPTANCE CRITERIA

Viewer clients do not use postgres_changes for chat or gifts.

Viral stream:

auto-start HLS at threshold

switch mass viewers to HLS

keep host/guests on WebRTC

Chat:

broadcast-first, durable store, catch-up works, slow mode works

Gifts:

transactional + idempotent, broadcast after commit, throttling works

Observability dashboards exist and load tests pass.

Implement in this order:

Remove viewer postgres_changes (chat/gifts) → broadcast-first

Transactional idempotent gifts + async aggregates

HLS egress + switching (hybrid delivery)

Caching + feed read models

Load tests + dashboards